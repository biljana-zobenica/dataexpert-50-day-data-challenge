{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dcada2-6c39-436a-955d-7effb21e997c",
   "metadata": {},
   "source": [
    "## Data Pipelines - Quiz 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b73bd-411f-401e-a4f4-04ef8e8dd6d9",
   "metadata": {},
   "source": [
    "### 1 - Idempotency and deduplication strategies are unnecessary if your messaging system guarantees exactly-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac41bad-dff7-4f97-b3ca-3e32cb947b41",
   "metadata": {},
   "source": [
    "False ✅\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc9f2d-44b1-43b9-8b66-702068b16912",
   "metadata": {},
   "source": [
    "### 2 - You are tasked with building a streaming data pipeline that ingests and processes telemetry data from IoT devices. The data varies in frequency and occasional spikes occur. What architectural solution would you implement to ensure reliability and scalability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c238df-de7d-4ffe-b18a-42bf239ac16a",
   "metadata": {},
   "source": [
    "Directly write to a database without any buffering to avoid additional latency.\n",
    "\n",
    "Use synchronous processing to ensure immediate handling of incoming data.\n",
    "\n",
    "Implement a message queue to buffer incoming telemetry data and handle spikes, with consumer groups to scale processing. ✅\n",
    "\n",
    "Use a single point of ingestion without scaling to save costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4739a1-ce01-4754-810e-a05995b870df",
   "metadata": {},
   "source": [
    "### 3 -You have deployed a data pipeline that feeds into a data warehouse, but stakeholders are reporting irregularities in the analytics results. What is your first step in diagnosing and resolving potential data quality issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565aeaa-36ed-4d35-8161-d5277e93af3b",
   "metadata": {},
   "source": [
    "Archive the existing data and start fresh with a new pipeline design.\n",
    "\n",
    "Implement automated data quality checks at multiple points in the pipeline and establish alerts for anomalies based on thresholds. ✅\n",
    "\n",
    "Replace all data in the warehouse and re-run the entire pipeline.\n",
    "\n",
    "Instruct analysts to double-check their queries for errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdd45d-43fe-41d4-836b-e97483921f66",
   "metadata": {},
   "source": [
    "### 4 - You are integrating a data lake into your existing data architecture but are concerned about ensuring governance and compliance for sensitive data. What architecture choice would best enhance data governance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f6aac-90cc-4f0c-bc3a-9eb000f0fa32",
   "metadata": {},
   "source": [
    "Use a single access control list for the entire data lake.\n",
    "\n",
    "Implement data classification at ingestion, alongside automated data discovery and lineage tracking to monitor sensitive data usage and compliance. ✅\n",
    "\n",
    "Store all data in the data lake without restriction, applying governance post-ingestion.\n",
    "\n",
    "Restrict access by deleting sensitive data entirely upon ingestion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
