{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08436524-f1ce-4925-9a2b-19480280ea26",
   "metadata": {},
   "source": [
    "## Distributed Computing (Spark, BigQuery, Snowflake) - Quiz 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc429e8-68dc-4a2b-9899-2e61bb014ce7",
   "metadata": {},
   "source": [
    "### 1 - Which configuration most directly controls the maximum delay for late data before state is cleaned up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14baff-5a6e-4377-9139-078688f1733e",
   "metadata": {},
   "source": [
    "spark.sql.streaming.stateStore.maintenanceInterval\n",
    "\n",
    "spark.streaming.backpressure.enabled\n",
    "\n",
    "spark.sql.shuffle.partitions\n",
    "\n",
    "Watermark ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc04ac-b0e4-4b75-b9a2-8c0d72b3e8ad",
   "metadata": {},
   "source": [
    "### 2 - For low-latency micro-batching, which config reduces end-to-end latency most directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984312d-e318-4df8-b735-6ce1b2d4b620",
   "metadata": {},
   "source": [
    "Increase spark.sql.shuffle.partitions\n",
    "\n",
    "Decrease trigger interval ✅\n",
    "\n",
    "Use checkpointing only on driver\n",
    "\n",
    "Increase driver memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451d707-4e94-4187-9183-2b795e4afb3f",
   "metadata": {},
   "source": [
    "### 3 - Which of the following guarantees does Spark Structured Streaming provide in micro-batch mode by default for file-based sinks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bee7bf-5ad4-4a05-ac17-9f32caae2aae",
   "metadata": {},
   "source": [
    "Exactly-once for all sinks\n",
    "\n",
    "Exactly-once if the sink is idempotent or supports atomic commits ✅\n",
    "\n",
    "No guarantees\n",
    "\n",
    "At-least-once always"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3ab42-f198-459d-8840-896f0c64cc7a",
   "metadata": {},
   "source": [
    "### 4 - Which join type is not supported in streaming-streaming (continuous) joins without special handling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a85d0e-eeca-4945-ba44-42cb07a28454",
   "metadata": {},
   "source": [
    "Equi-join with watermark on both sides\n",
    "\n",
    "Left outer join with unbounded state ✅\n",
    "\n",
    "Inner join with time constraints\n",
    "\n",
    "Stream–static join (stream vs table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa2550-b793-4dfa-89c0-d7adc79d7d37",
   "metadata": {},
   "source": [
    "### 5 - Using foreachBatch, a developer writes each micro‑batch to a PostgreSQL table with INSERT … ON CONFLICT DO UPDATE. Occasionally, after a task retry, duplicate rows appear. Which addition will make the sink idempotent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25e401-48ab-4ba7-87b4-7974880e2ff0",
   "metadata": {},
   "source": [
    "Disable checkpointing for the query.\n",
    "\n",
    "Switch to append mode.\n",
    "\n",
    "Include the batchId in the primary key of the target table and use it in the upsert. ✅\n",
    "\n",
    "Increase spark.sql.streaming.maxFilesPerTrigger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
